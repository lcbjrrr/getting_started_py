{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/OjE+8GQ5afs6tSJRBBNU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Given a Pandas DataFrame, remove duplicate rows and  reset the index of the DataFrame\n"],"metadata":{"id":"Gc3xF_KWDb9x"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.decomposition import PCA\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from scipy.stats import mstats\n","\n","# 1. Given a Pandas DataFrame, remove duplicate rows and reset the index of the DataFrame.\n","def remove_duplicates_and_reset_index(df):\n","    \"\"\"\n","    Removes duplicate rows from a Pandas DataFrame and resets its index.\n","\n","    Args:\n","        df (pd.DataFrame): The input Pandas DataFrame.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with duplicates removed and index reset.\n","    \"\"\"\n","    df = df.drop_duplicates()\n","    df = df.reset_index(drop=True)  # drop=True prevents adding the old index as a new column\n","    return df\n","\n","# Sample Usage:\n","data = {'col1': [1, 2, 2, 3, 4, 4, 5], 'col2': ['a', 'b', 'b', 'c', 'd', 'd', 'e']}\n","df = pd.DataFrame(data)\n","df_cleaned = remove_duplicates_and_reset_index(df)\n","print(\"DataFrame after removing duplicates and resetting index:\\n\", df_cleaned)\n"],"metadata":{"id":"rYFfCUK6DcDg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement a program that reads a CSV file into a Pandas  DataFrame and handles missing values using Imputation+\n"],"metadata":{"id":"o80VnfL9DcJg"}},{"cell_type":"code","source":["\n","\n","# 2. Implement a program that reads a CSV file into a Pandas DataFrame and handles missing values using Imputation.\n","def handle_missing_values_with_imputation(csv_file, strategy='mean'):\n","    \"\"\"\n","    Reads a CSV file into a Pandas DataFrame and handles missing values using imputation.\n","\n","    Args:\n","        csv_file (str): Path to the CSV file.\n","        strategy (str, optional): Imputation strategy. Can be 'mean', 'median', 'most_frequent', or 'constant'.\n","                                 Defaults to 'mean'.  If 'constant', use `fill_value`.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with missing values imputed.\n","    \"\"\"\n","    try:\n","        df = pd.read_csv(csv_file)\n","        imputer = SimpleImputer(strategy=strategy)\n","        df.iloc[:, :] = imputer.fit_transform(df)  # Impute all columns (can be adjusted)\n","        return df\n","    except FileNotFoundError:\n","        print(f\"Error: File '{csv_file}' not found.\")\n","        return None\n","\n","# Sample CSV file creation:\n","data = {'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, 9, 10], 'C': [11, 12, 13, np.nan, 15]}\n","df = pd.DataFrame(data)\n","df.to_csv('missing_data.csv', index=False)\n","\n","df_imputed = handle_missing_values_with_imputation('missing_data.csv', strategy='mean')\n","if df_imputed is not None:\n","    print(\"\\nDataFrame after imputation:\\n\", df_imputed)\n","\n"],"metadata":{"id":"qRA6B0t9DcPP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Create a function that takes a Pandas DataFrame and  converts text data into numerical values using One-Hot  Encoding\n"],"metadata":{"id":"5H94pUReDcUt"}},{"cell_type":"code","source":["\n","# 3. Create a function that takes a Pandas DataFrame and converts text data into numerical values using One-Hot Encoding.\n","def one_hot_encode_categorical_data(df, columns_to_encode=None):\n","    \"\"\"\n","    Converts text data into numerical values using One-Hot Encoding.\n","\n","    Args:\n","        df (pd.DataFrame): The input Pandas DataFrame.\n","        columns_to_encode (list, optional): List of column names to encode. If None, encodes all object type columns.\n","                                          Defaults to None.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with One-Hot Encoded columns.\n","    \"\"\"\n","\n","    if columns_to_encode is None:\n","        columns_to_encode = df.select_dtypes(include=['object']).columns\n","    df = pd.get_dummies(df, columns=columns_to_encode, drop_first=True) #drop_first avoids multicollinearity\n","    return df\n","\n","# Sample Usage:\n","data = {'Product': ['A', 'B', 'A', 'C', 'B'], 'Color': ['Red', 'Blue', 'Green', 'Red', 'Blue'], 'Price': [10, 20, 15, 25, 18]}\n","df = pd.DataFrame(data)\n","df_encoded = one_hot_encode_categorical_data(df)\n","print(\"\\nDataFrame after One-Hot Encoding:\\n\", df_encoded)\n"],"metadata":{"id":"AGeMaHGhDcaK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Given a Pandas DataFrame, normalize the numerical  features using Z-Score Normalization\n"],"metadata":{"id":"AUTisinrDcgi"}},{"cell_type":"code","source":["\n","\n","# 4. Given a Pandas DataFrame, normalize the numerical features using Z-Score Normalization.\n","def normalize_numerical_features_zscore(df, columns_to_normalize=None):\n","    \"\"\"\n","    Normalizes numerical features using Z-Score Normalization (StandardScaler).\n","\n","    Args:\n","        df (pd.DataFrame): The input Pandas DataFrame.\n","        columns_to_normalize (list, optional): List of column names to normalize. If None, normalizes all numeric columns.\n","                                             Defaults to None.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with normalized numerical features.\n","    \"\"\"\n","\n","    if columns_to_normalize is None:\n","        columns_to_normalize = df.select_dtypes(include=np.number).columns\n","    scaler = StandardScaler()\n","    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n","    return df\n","\n","# Sample Usage:\n","data = {'A': [10, 20, 30, 40, 50], 'B': [100, 200, 150, 250, 180], 'Category': ['X', 'Y', 'X', 'Z', 'Y']}\n","df = pd.DataFrame(data)\n","df_normalized = normalize_numerical_features_zscore(df)\n","print(\"\\nDataFrame after Z-Score Normalization:\\n\", df_normalized)\n"],"metadata":{"id":"Hb8oMC6rDcma"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write a Python program that uses Scikit-Learn to perform  data standardization on a dataset\n"],"metadata":{"id":"LaWlSJwYDcsR"}},{"cell_type":"code","source":["\n","\n","# 5. Write a Python program that uses Scikit-Learn to perform data standardization on a dataset.\n","def standardize_data_sklearn(df, columns_to_standardize=None):\n","    \"\"\"\n","    Performs data standardization using Scikit-Learn's StandardScaler.\n","\n","    Args:\n","        df (pd.DataFrame): The input Pandas DataFrame.\n","        columns_to_standardize (list, optional): List of column names to standardize. If None, standardizes all numeric columns.\n","                                               Defaults to None.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with standardized data.\n","    \"\"\"\n","\n","    if columns_to_standardize is None:\n","        columns_to_standardize = df.select_dtypes(include=np.number).columns\n","    scaler = StandardScaler()\n","    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n","    return df\n","\n","# Sample Usage: (Reusing df from previous example)\n","df_standardized = standardize_data_sklearn(df)\n","print(\"\\nDataFrame after Standardization (using Scikit-Learn):\\n\", df_standardized)\n"],"metadata":{"id":"q8aZ_Z0_DcyK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement a program that reads a JSON file into a Pandas  DataFrame and handles outliers using Winsorization\n"],"metadata":{"id":"7UvouIXCDc32"}},{"cell_type":"code","source":["\n","\n","# 6. Implement a program that reads a JSON file into a Pandas DataFrame and handles outliers using Winsorization.\n","import json\n","from scipy.stats import mstats\n","\n","def handle_outliers_with_winsorization(json_file, columns_to_winsorize=None, limits=(0.05, 0.95)):\n","    \"\"\"\n","    Reads a JSON file into a Pandas DataFrame and handles outliers using Winsorization.\n","\n","    Args:\n","        json_file (str): Path to the JSON file.\n","        columns_to_winsorize (list, optional): List of column names to winsorize. If None, winsorizes all numeric columns.\n","                                            Defaults to None.\n","        limits (tuple, optional): Tuple specifying the lower and upper percentiles for winsorization. Defaults to (0.05, 0.95).\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with outliers handled using Winsorization.\n","    \"\"\"\n","\n","    try:\n","        with open(json_file, 'r') as f:\n","            data = json.load(f)\n","        df = pd.DataFrame(data)\n","\n","        if columns_to_winsorize is None:\n","            columns_to_winsorize = df.select_dtypes(include=np.number).columns\n","\n","        for col in columns_to_winsorize:\n","            df[col] = mstats.winsorize(df[col], limits=limits)\n","        return df\n","\n","    except FileNotFoundError:\n","        print(f\"Error: File '{json_file}' not found.\")\n","        return None\n","    except json.JSONDecodeError:\n","        print(f\"Error: Invalid JSON format in '{json_file}'.\")\n","        return None\n","\n","# Sample JSON file creation:\n","json_data = {'A': [10, 20, 100, 40, 500], 'B': [100, 200, 150, -50, 180], 'C': [5, 10, 15, 20, 100]}\n","with open('outlier_data.json', 'w') as f:\n","    json.dump(json_data, f)\n","\n","df_winsorized = handle_outliers_with_winsorization('outlier_data.json', limits=(0.1, 0.9))\n","if df_winsorized is not None:\n","    print(\"\\nDataFrame after Winsorization:\\n\", df_winsorized)\n","\n"],"metadata":{"id":"BvtJYxM2Dc9r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a function that takes a Pandas DataFrame and  removes irrelevant features using Feature Selection  techniques.\n"],"metadata":{"id":"84A606PJDdD1"}},{"cell_type":"code","source":["\n","# 7. Create a function that takes a Pandas DataFrame and removes irrelevant features using Feature Selection techniques.\n","def remove_irrelevant_features(df, target_col, k=5):\n","    \"\"\"\n","    Removes irrelevant features using Feature Selection techniques (SelectKBest with f_classif for classification).\n","\n","    Args:\n","        df (pd.DataFrame): The input Pandas DataFrame.\n","        target_col (str): The name of the target column.\n","        k (int, optional): The number of top features to select. Defaults to 5.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with selected features.\n","    \"\"\"\n","\n","    X = df.drop(columns=[target_col])\n","    y = df[target_col]\n","\n","    # Handle non-numeric features (important for f_classif)\n","    X = pd.get_dummies(X, drop_first=True)  # One-hot encode categorical features\n","\n","    selector = SelectKBest(f_classif, k=k)\n","    X_new = selector.fit_transform(X, y)\n","\n","    selected_features = X.columns[selector.get_support()]\n","    df_selected = df[selected_features.tolist() + [target_col]] # Keep the target column\n","\n","    return df_selected\n","\n","# Sample Usage:\n","data = {'feature1': [1, 2, 3, 4, 5],\n","        'feature2': [5, 4, 3, 2, 1],\n","        'category1': ['A', 'B', 'A', 'C', 'B'],\n","        'category2': ['X', 'Y', 'X', 'Z', 'Y'],\n","        'target': [0, 1, 0, 1, 0]}\n","df = pd.DataFrame(data)\n","\n","df_feature_selected = remove_irrelevant_features(df, 'target', k=2)\n","print(\"\\nDataFrame after Feature Selection:\\n\", df_feature_selected)\n"],"metadata":{"id":"eyoSlVF4DdJZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Given a CSV file with customer details, preprocess the  data for further analysis (e.g., handle missing values, scale  features)\n"],"metadata":{"id":"LJ-Dkh0bDdPb"}},{"cell_type":"code","source":["\n","\n","# 8. Given a CSV file with customer details, preprocess the data for further analysis (e.g., handle missing values, scale features).\n","def preprocess_customer_data(csv_file):\n","    \"\"\"\n","    Preprocesses customer data from a CSV file for further analysis.\n","    Handles missing values, scales numerical features, and encodes categorical features.\n","\n","    Args:\n","        csv_file (str): Path to the CSV file.\n","\n","    Returns:\n","        pd.DataFrame: The preprocessed Pandas DataFrame, or None if file not found.\n","    \"\"\"\n","\n","    try:\n","        df = pd.read_csv(csv_file)\n","\n","        # Handle Missing Values (Example - customize as needed)\n","        for col in df.columns:\n","            if df[col].dtype == 'object':\n","                df[col] = df[col].fillna(df[col].mode()[0])  # Impute categorical with mode\n","            else:\n","                df[col] = df[col].fillna(df[col].median())  # Impute numerical with median\n","\n","        # Scale Numerical Features (Example - customize)\n","        numerical_cols = df.select_dtypes(include=np.number).columns\n","        numerical_cols = numerical_cols.drop(labels=['CustomerID'], errors='ignore') # Assuming 'CustomerID' is an ID, not for scaling\n","        df[numerical_cols] = StandardScaler().fit_transform(df[numerical_cols])\n","\n","        # Encode Categorical Features (Example - customize)\n","        categorical_cols = df.select_dtypes(include=['object']).columns\n","        df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n","\n","        return df\n","\n","    except FileNotFoundError:\n","        print(f\"Error: File '{csv_file}' not found.\")\n","        return None\n","\n","# Sample CSV file creation:\n","customer_data = {'CustomerID': [1, 2, 3, 4, 5],\n","                 'Age': [25, 30, np.nan, 40, 35],\n","                 'City': ['New York', 'London', 'Paris', 'New York', np.nan],\n","                 'Salary': [50000, 60000, 55000, np.nan, 70000],\n","                 'Gender': ['Male', 'Female', 'Male', 'Female', 'Male']}\n","customer_df = pd.DataFrame(customer_data)\n","customer_df.to_csv('customer_data.csv', index=False)\n","\n","df_preprocessed_customer = preprocess_customer_data('customer_data.csv')\n","if df_preprocessed_customer is not None:\n","    print(\"\\nPreprocessed Customer Data:\\n\", df_preprocessed_customer)\n"],"metadata":{"id":"cAfoFCstDdVS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write a Python program that uses Scikit-Learn to perform  data transformation using PCA (Principal Component  Analysis)\n"],"metadata":{"id":"GmeDcIiNDdbS"}},{"cell_type":"code","source":["\n","\n","# 9. Write a Python program that uses Scikit-Learn to perform data transformation using PCA (Principal Component Analysis).\n","def apply_pca(df, n_components=2, columns_to_use=None):\n","    \"\"\"\n","    Applies Principal Component Analysis (PCA) to reduce the dimensionality of the data.\n","\n","    Args:\n","        df (pd.DataFrame): The input Pandas DataFrame.\n","        n_components (int, optional): The number of principal components to retain. Defaults to 2.\n","        columns_to_use (list, optional): List of column names to use for PCA. If None, uses all numeric columns.\n","                                        Defaults to None.\n","\n","    Returns:\n","        pd.DataFrame: A new Pandas DataFrame containing the principal components.\n","    \"\"\"\n","\n","    if columns_to_use is None:\n","        columns_to_use = df.select_dtypes(include=np.number).columns\n","\n","    pca = PCA(n_components=n_components)\n","    principal_components = pca.fit_transform(df[columns_to_use])\n","    pc_cols = [f'PC{i+1}' for i in range(n_components)]  # Create column names like PC1, PC2\n","    df_pca = pd.DataFrame(data=principal_components, columns=pc_cols)\n","    return df_pca\n","\n","# Sample Usage: (Using the customer_df from previous example, but after preprocessing)\n","df_pca = apply_pca(df_preprocessed_customer, n_components=2)\n","print(\"\\nDataFrame after PCA:\\n\", df_pca)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"UQ7I2pNVDdgv","executionInfo":{"status":"error","timestamp":1744244406058,"user_tz":300,"elapsed":14,"user":{"displayName":"Luiz Barboza","userId":"00340271388648462291"}},"outputId":"950a3785-0299-4975-9e51-13727603e3cd"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df_preprocessed_customer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-a7bb105bd426>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Sample Usage: (Using the customer_df from previous example, but after preprocessing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdf_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_pca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_preprocessed_customer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDataFrame after PCA:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_preprocessed_customer' is not defined"]}]},{"cell_type":"markdown","source":["Implement a function that takes a Pandas DataFrame and  performs data discretization on a numerical feature\n"],"metadata":{"id":"uhmxP4V-DdnA"}},{"cell_type":"code","source":["\n","# 10. Implement a function that takes a Pandas DataFrame and performs data discretization on a numerical feature.\n","def discretize_numerical_feature(df, col_to_discretize, bins=5, labels=None, method='equal_width'):\n","    \"\"\"\n","    Performs data discretization on a numerical feature.\n","\n","    Args:\n","        df (pd.DataFrame): The input Pandas DataFrame.\n","        col_to_discretize (str): The name of the numerical column to discretize.\n","        bins (int or list, optional): The number of equal-width bins or a list of bin edges. Defaults to 5.\n","        labels (list, optional): Labels to assign to the bins. If None, uses default bin labels. Defaults to None.\n","        method (str, optional): Discretization method. Can be 'equal_width', 'equal_freq', or 'custom'.\n","                               Defaults to 'equal_width'. If 'custom', use `bins` as bin edges.\n","\n","    Returns:\n","        pd.DataFrame: The DataFrame with the discretized feature added as a new column.\n","    \"\"\"\n","\n","    if method == 'equal_width':\n","        df[f'{col_to_discretize}_binned'] = pd.cut(df[col_to_discretize], bins=bins, labels=labels)\n","    elif method == 'equal_freq':\n","        df[f'{col_to_discretize}_binned'] = pd.qcut(df[col_to_discretize], q=bins, labels=labels)\n","    elif method == 'custom':\n","        df[f'{col_to_discretize}_binned'] = pd.cut(df[col_to_discretize], bins=bins, labels=labels, right=False) #right=False: bins[i-1] < x <= bins[i]\n","    else:\n","        raise ValueError(\"Invalid discretization method. Choose 'equal_width', 'equal_freq', or 'custom'.\")\n","\n","    return df\n","\n","# Sample Usage: (Using customer_df again)\n","df_discretized = discretize_numerical_feature(df_preprocessed_customer.copy(), 'Age', bins=3, labels=['Young', 'Middle-Aged', 'Senior'], method='equal_width')\n","print(\"\\nDataFrame after Discret\",df_discretized)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"Caa0nHrdDduI","executionInfo":{"status":"error","timestamp":1744244397158,"user_tz":300,"elapsed":65,"user":{"displayName":"Luiz Barboza","userId":"00340271388648462291"}},"outputId":"a0c4aad9-59c6-4f96-bda5-52026644fd94"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df_preprocessed_customer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8259f8c39a92>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Sample Usage: (Using customer_df again)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdf_discretized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscretize_numerical_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_preprocessed_customer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Young'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Middle-Aged'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Senior'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'equal_width'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDataFrame after Discret\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_discretized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_preprocessed_customer' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5rGb6TvAEaw3","executionInfo":{"status":"ok","timestamp":1744244420788,"user_tz":300,"elapsed":4,"user":{"displayName":"Luiz Barboza","userId":"00340271388648462291"}}},"execution_count":2,"outputs":[]}]}